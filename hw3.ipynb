{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "\n",
    "# from LFWDataset import LFWDataset\n",
    "# from SiameseNet import SiameseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "class LFWDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Faces in the wild dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir='lfw/', train=False, test=False, transform=None):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.root_dir = root_dir\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        if (self.train and self.test) or not (self.train or self.test):\n",
    "            raise ValueError('Exactly one of train and test must be set.')\n",
    "        \n",
    "        \"\"\"\n",
    "        Getting Train/Test splits\n",
    "        \"\"\"\n",
    "        dataset = set()\n",
    "        if self.train:\n",
    "            filename='train.txt'\n",
    "        else:\n",
    "            filename='test.txt'\n",
    "\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                line = line.split()\n",
    "                self.files.append(line[:2])\n",
    "                self.labels.append(int(line[2]))\n",
    "                \n",
    "        #print(self.files)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "#         return sum([len(files) for r, d, files in os.walk(self.root_dir)])\n",
    "#         https://stackoverflow.com/questions/16910330/return-number-of-files-in-directory-and-subdirectory\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im_names = self.files[idx]\n",
    "        im1 = Image.open(os.path.join(self.root_dir, im_names[0]))\n",
    "        im2 = Image.open(os.path.join(self.root_dir, im_names[1]))\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            im1 = self.transform(im1)\n",
    "            im2 = self.transform(im2)\n",
    "            \n",
    "        return im1, im2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 5, stride=(1,1), padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d((2,2), stride=(2,2)),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 5, stride=(1,1), padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d((2,2), stride=(2,2)),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, stride=(1,1), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d((2,2), stride=(2,2)),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, stride=(1,1), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(512 * 16 * 16, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(1024)\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(2*1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn(x)\n",
    "        #flatten\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output        \n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        f1 = self.forward_once(input1)\n",
    "        f2 = self.forward_once(input2)\n",
    "#         print(f1, f2)\n",
    "        output = self.fc2(torch.cat((f1, f2), 1))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def show(img, filename=None, save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    plt.show()\n",
    "    if save and filename is not None:\n",
    "        plt.savefig(filename)\n",
    "\n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = LFWDataset(train=True,\n",
    "                      transform=transforms.Compose([transforms.Scale((128,128)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ]))\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_dataloader = DataLoader(trainset,\n",
    "#                         shuffle=True,\n",
    "#                         num_workers=8,\n",
    "#                         batch_size=8)\n",
    "# dataiter = iter(vis_dataloader)\n",
    "\n",
    "# example_batch = next(dataiter)\n",
    "# concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "\n",
    "# show(torchvision.utils.make_grid(concatenated))\n",
    "# print(example_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNet().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-6\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiameseNet (\n",
      "  (cnn): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): ReLU (inplace)\n",
      "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU (inplace)\n",
      "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (11): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU (inplace)\n",
      "    (14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (fc1): Sequential (\n",
      "    (0): Linear (131072 -> 1024)\n",
      "    (1): ReLU (inplace)\n",
      "    (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (fc2): Sequential (\n",
      "    (0): Linear (2048 -> 1)\n",
      "    (1): Sigmoid ()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number= 0\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      " Current loss 0.701419591904\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.688651204109\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.682772099972\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.722647666931\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.648253560066\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.6255428195\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.627396821976\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.610341310501\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 0.612944424152\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 0.582180857658\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 0.581018149853\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 0.596436202526\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 0.538316965103\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 0.535675048828\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 0.543514728546\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 0.536738932133\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 0.535087823868\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 0.531438529491\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 0.513570189476\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 0.522263407707\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 0.488791406155\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 0.50958365202\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 0.4926892519\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 0.505516648293\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 0.462774157524\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 0.462058246136\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 0.466639399529\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 0.481886416674\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 0.461082100868\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 0.466592758894\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 0.459093540907\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 0.470825314522\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 0.45574966073\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 0.434043437243\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 0.452100425959\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 0.458604037762\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 0.437871396542\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 0.440394669771\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 0.441013902426\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 0.429612517357\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 0.416730612516\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 0.431685864925\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 0.403901606798\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 0.429253131151\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 0.400723963976\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 0.411111593246\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 0.405524641275\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 0.413294970989\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 0.399137973785\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 0.395644277334\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 0.412992715836\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 0.408937364817\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 0.393723428249\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 0.389089345932\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 0.405726790428\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 0.391155034304\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 0.386320352554\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 0.385948300362\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 0.387629777193\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 0.397384107113\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 0.380385994911\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 0.389890521765\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 0.381880640984\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 0.385775268078\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 0.387944847345\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 0.377500474453\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 0.38118314743\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 0.391230374575\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 0.372104614973\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 0.371360272169\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 0.375135600567\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 0.37288826704\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 0.368736833334\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 0.365672826767\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 0.364179223776\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 0.371530890465\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 0.366676211357\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 0.368591994047\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 0.359752982855\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 0.35937204957\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 0.364768415689\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 0.361282616854\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 0.359496593475\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 0.358392477036\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 0.36024671793\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 0.356039911509\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 0.359219193459\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 0.361152619123\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 0.356286108494\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 0.369928956032\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 0.35497635603\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 0.363196611404\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 0.355893969536\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 0.359855115414\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 0.347090512514\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 0.356274008751\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 0.349300116301\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 0.35864764452\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 0.358886361122\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 0.350763559341\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 0.349984258413\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 0.351964503527\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 0.352912783623\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 0.351581573486\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 0.34440356493\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 0.353318750858\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 0.371378064156\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 0.355012744665\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 0.352915793657\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 0.351164579391\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 0.348864406347\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 0.346369355917\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 0.353344380856\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 0.342509120703\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 0.344606667757\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 0.344195902348\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 0.343466132879\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 0.35220259428\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 0.342318236828\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 0.360675662756\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 0.344536215067\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 0.340612620115\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 0.351360470057\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 0.33918979764\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 0.340906322002\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 0.339455127716\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 0.342434585094\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 0.338688254356\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 0.339913755655\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 0.340346723795\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 0.345415174961\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 0.342149764299\n",
      "\n",
      "Epoch number 33\n",
      " Current loss 0.341095685959\n",
      "\n",
      "Epoch number 33\n",
      " Current loss 0.341688990593\n",
      "\n",
      "Epoch number 33\n",
      " Current loss 0.345419168472\n",
      "\n",
      "Epoch number 33\n",
      " Current loss 0.344604998827\n",
      "\n",
      "Epoch number 34\n",
      " Current loss 0.337856948376\n",
      "\n",
      "Epoch number 34\n",
      " Current loss 0.335103452206\n",
      "\n",
      "Epoch number 34\n",
      " Current loss 0.336558818817\n",
      "\n",
      "Epoch number 34\n",
      " Current loss 0.339336544275\n",
      "\n",
      "Epoch number 35\n",
      " Current loss 0.335617959499\n",
      "\n",
      "Epoch number 35\n",
      " Current loss 0.334718614817\n",
      "\n",
      "Epoch number 35\n",
      " Current loss 0.339837223291\n",
      "\n",
      "Epoch number 35\n",
      " Current loss 0.338326513767\n",
      "\n",
      "Epoch number 36\n",
      " Current loss 0.335939526558\n",
      "\n",
      "Epoch number 36\n",
      " Current loss 0.33298394084\n",
      "\n",
      "Epoch number 36\n",
      " Current loss 0.341896146536\n",
      "\n",
      "Epoch number 36\n",
      " Current loss 0.339992880821\n",
      "\n",
      "Epoch number 37\n",
      " Current loss 0.334149748087\n",
      "\n",
      "Epoch number 37\n",
      " Current loss 0.337949603796\n",
      "\n",
      "Epoch number 37\n",
      " Current loss 0.336106181145\n",
      "\n",
      "Epoch number 37\n",
      " Current loss 0.336245507002\n",
      "\n",
      "Epoch number 38\n",
      " Current loss 0.334316790104\n",
      "\n",
      "Epoch number 38\n",
      " Current loss 0.332955777645\n",
      "\n",
      "Epoch number 38\n",
      " Current loss 0.331945329905\n",
      "\n",
      "Epoch number 38\n",
      " Current loss 0.344131231308\n",
      "\n",
      "Epoch number 39\n",
      " Current loss 0.337804377079\n",
      "\n",
      "Epoch number 39\n",
      " Current loss 0.335277348757\n",
      "\n",
      "Epoch number 39\n",
      " Current loss 0.330441594124\n",
      "\n",
      "Epoch number 39\n",
      " Current loss 0.330907732248\n",
      "\n",
      "Epoch number 40\n",
      " Current loss 0.333906203508\n",
      "\n",
      "Epoch number 40\n",
      " Current loss 0.334423720837\n",
      "\n",
      "Epoch number 40\n",
      " Current loss 0.331360071898\n",
      "\n",
      "Epoch number 40\n",
      " Current loss 0.350845277309\n",
      "\n",
      "Epoch number 41\n",
      " Current loss 0.335773229599\n",
      "\n",
      "Epoch number 41\n",
      " Current loss 0.337968349457\n",
      "\n",
      "Epoch number 41\n",
      " Current loss 0.333003550768\n",
      "\n",
      "Epoch number 41\n",
      " Current loss 0.331774532795\n",
      "\n",
      "Epoch number 42\n",
      " Current loss 0.332760363817\n",
      "\n",
      "Epoch number 42\n",
      " Current loss 0.337997406721\n",
      "\n",
      "Epoch number 42\n",
      " Current loss 0.336508482695\n",
      "\n",
      "Epoch number 42\n",
      " Current loss 0.330349028111\n",
      "\n",
      "Epoch number 43\n",
      " Current loss 0.329183787107\n",
      "\n",
      "Epoch number 43\n",
      " Current loss 0.329553037882\n",
      "\n",
      "Epoch number 43\n",
      " Current loss 0.332217484713\n",
      "\n",
      "Epoch number 43\n",
      " Current loss 0.332654595375\n",
      "\n",
      "Epoch number 44\n",
      " Current loss 0.328458249569\n",
      "\n",
      "Epoch number 44\n",
      " Current loss 0.327164888382\n",
      "\n",
      "Epoch number 44\n",
      " Current loss 0.333598941565\n",
      "\n",
      "Epoch number 44\n",
      " Current loss 0.331193774939\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 45\n",
      " Current loss 0.333851933479\n",
      "\n",
      "Epoch number 45\n",
      " Current loss 0.326376914978\n",
      "\n",
      "Epoch number 45\n",
      " Current loss 0.332216382027\n",
      "\n",
      "Epoch number 45\n",
      " Current loss 0.327900677919\n",
      "\n",
      "Epoch number 46\n",
      " Current loss 0.328181117773\n",
      "\n",
      "Epoch number 46\n",
      " Current loss 0.32725957036\n",
      "\n",
      "Epoch number 46\n",
      " Current loss 0.329260438681\n",
      "\n",
      "Epoch number 46\n",
      " Current loss 0.328396111727\n",
      "\n",
      "Epoch number 47\n",
      " Current loss 0.332577228546\n",
      "\n",
      "Epoch number 47\n",
      " Current loss 0.327203035355\n",
      "\n",
      "Epoch number 47\n",
      " Current loss 0.327204674482\n",
      "\n",
      "Epoch number 47\n",
      " Current loss 0.330403447151\n",
      "\n",
      "Epoch number 48\n",
      " Current loss 0.342728465796\n",
      "\n",
      "Epoch number 48\n",
      " Current loss 0.326999098063\n",
      "\n",
      "Epoch number 48\n",
      " Current loss 0.326657474041\n",
      "\n",
      "Epoch number 48\n",
      " Current loss 0.32992914319\n",
      "\n",
      "Epoch number 49\n",
      " Current loss 0.334392279387\n",
      "\n",
      "Epoch number 49\n",
      " Current loss 0.327635020018\n",
      "\n",
      "Epoch number 49\n",
      " Current loss 0.325817167759\n",
      "\n",
      "Epoch number 49\n",
      " Current loss 0.326175630093\n",
      "\n",
      "Epoch number 50\n",
      " Current loss 0.328720599413\n",
      "\n",
      "Epoch number 50\n",
      " Current loss 0.329571038485\n",
      "\n",
      "Epoch number 50\n",
      " Current loss 0.327218830585\n",
      "\n",
      "Epoch number 50\n",
      " Current loss 0.326904594898\n",
      "\n",
      "Epoch number 51\n",
      " Current loss 0.326261758804\n",
      "\n",
      "Epoch number 51\n",
      " Current loss 0.327695608139\n",
      "\n",
      "Epoch number 51\n",
      " Current loss 0.323829740286\n",
      "\n",
      "Epoch number 51\n",
      " Current loss 0.327492505312\n",
      "\n",
      "Epoch number 52\n",
      " Current loss 0.327641099691\n",
      "\n",
      "Epoch number 52\n",
      " Current loss 0.325998991728\n",
      "\n",
      "Epoch number 52\n",
      " Current loss 0.327129691839\n",
      "\n",
      "Epoch number 52\n",
      " Current loss 0.326333642006\n",
      "\n",
      "Epoch number 53\n",
      " Current loss 0.32701036334\n",
      "\n",
      "Epoch number 53\n",
      " Current loss 0.327162325382\n",
      "\n",
      "Epoch number 53\n",
      " Current loss 0.329149186611\n",
      "\n",
      "Epoch number 53\n",
      " Current loss 0.325397133827\n",
      "\n",
      "Epoch number 54\n",
      " Current loss 0.329604119062\n",
      "\n",
      "Epoch number 54\n",
      " Current loss 0.326862156391\n",
      "\n",
      "Epoch number 54\n",
      " Current loss 0.326066195965\n",
      "\n",
      "Epoch number 54\n",
      " Current loss 0.325333833694\n",
      "\n",
      "Epoch number 55\n",
      " Current loss 0.327365368605\n",
      "\n",
      "Epoch number 55\n",
      " Current loss 0.324243217707\n",
      "\n",
      "Epoch number 55\n",
      " Current loss 0.328616857529\n",
      "\n",
      "Epoch number 55\n",
      " Current loss 0.326831519604\n",
      "\n",
      "Epoch number 56\n",
      " Current loss 0.325590282679\n",
      "\n",
      "Epoch number 56\n",
      " Current loss 0.327838987112\n",
      "\n",
      "Epoch number 56\n",
      " Current loss 0.323209434748\n",
      "\n",
      "Epoch number 56\n",
      " Current loss 0.326860159636\n",
      "\n",
      "Epoch number 57\n",
      " Current loss 0.328798055649\n",
      "\n",
      "Epoch number 57\n",
      " Current loss 0.324412286282\n",
      "\n",
      "Epoch number 57\n",
      " Current loss 0.323752552271\n",
      "\n",
      "Epoch number 57\n",
      " Current loss 0.33072912693\n",
      "\n",
      "Epoch number 58\n",
      " Current loss 0.326897382736\n",
      "\n",
      "Epoch number 58\n",
      " Current loss 0.324320703745\n",
      "\n",
      "Epoch number 58\n",
      " Current loss 0.322446137667\n",
      "\n",
      "Epoch number 58\n",
      " Current loss 0.325464725494\n",
      "\n",
      "Epoch number 59\n",
      " Current loss 0.322482228279\n",
      "\n",
      "Epoch number 59\n",
      " Current loss 0.324370533228\n",
      "\n",
      "Epoch number 59\n",
      " Current loss 0.323717921972\n",
      "\n",
      "Epoch number 59\n",
      " Current loss 0.325164526701\n",
      "\n",
      "Epoch number 60\n",
      " Current loss 0.322531938553\n",
      "\n",
      "Epoch number 60\n",
      " Current loss 0.326372861862\n",
      "\n",
      "Epoch number 60\n",
      " Current loss 0.324617952108\n",
      "\n",
      "Epoch number 60\n",
      " Current loss 0.322589486837\n",
      "\n",
      "Epoch number 61\n",
      " Current loss 0.323374390602\n",
      "\n",
      "Epoch number 61\n",
      " Current loss 0.32253408432\n",
      "\n",
      "Epoch number 61\n",
      " Current loss 0.324096173048\n",
      "\n",
      "Epoch number 61\n",
      " Current loss 0.323837935925\n",
      "\n",
      "Epoch number 62\n",
      " Current loss 0.322584092617\n",
      "\n",
      "Epoch number 62\n",
      " Current loss 0.324994206429\n",
      "\n",
      "Epoch number 62\n",
      " Current loss 0.324284672737\n",
      "\n",
      "Epoch number 62\n",
      " Current loss 0.323233753443\n",
      "\n",
      "Epoch number 63\n",
      " Current loss 0.33919146657\n",
      "\n",
      "Epoch number 63\n",
      " Current loss 0.333476513624\n",
      "\n",
      "Epoch number 63\n",
      " Current loss 0.323383539915\n",
      "\n",
      "Epoch number 63\n",
      " Current loss 0.324301153421\n",
      "\n",
      "Epoch number 64\n",
      " Current loss 0.323102295399\n",
      "\n",
      "Epoch number 64\n",
      " Current loss 0.32350191474\n",
      "\n",
      "Epoch number 64\n",
      " Current loss 0.327609658241\n",
      "\n",
      "Epoch number 64\n",
      " Current loss 0.325838804245\n",
      "\n",
      "Epoch number 65\n",
      " Current loss 0.327232807875\n",
      "\n",
      "Epoch number 65\n",
      " Current loss 0.327577650547\n",
      "\n",
      "Epoch number 65\n",
      " Current loss 0.321712225676\n",
      "\n",
      "Epoch number 65\n",
      " Current loss 0.322287619114\n",
      "\n",
      "Epoch number 66\n",
      " Current loss 0.322457462549\n",
      "\n",
      "Epoch number 66\n",
      " Current loss 0.321962624788\n",
      "\n",
      "Epoch number 66\n",
      " Current loss 0.325767248869\n",
      "\n",
      "Epoch number 66\n",
      " Current loss 0.322478175163\n",
      "\n",
      "Epoch number 67\n",
      " Current loss 0.323388040066\n",
      "\n",
      "Epoch number 67\n",
      " Current loss 0.321430146694\n",
      "\n",
      "Epoch number 67\n",
      " Current loss 0.321990579367\n",
      "\n",
      "Epoch number 67\n",
      " Current loss 0.321051597595\n",
      "\n",
      "Epoch number 68\n",
      " Current loss 0.32305932045\n",
      "\n",
      "Epoch number 68\n",
      " Current loss 0.321924507618\n",
      "\n",
      "Epoch number 68\n",
      " Current loss 0.322749167681\n",
      "\n",
      "Epoch number 68\n",
      " Current loss 0.324436575174\n",
      "\n",
      "Epoch number 69\n",
      " Current loss 0.327908217907\n",
      "\n",
      "Epoch number 69\n",
      " Current loss 0.321251809597\n",
      "\n",
      "Epoch number 69\n",
      " Current loss 0.322233855724\n",
      "\n",
      "Epoch number 69\n",
      " Current loss 0.320715457201\n",
      "\n",
      "Epoch number 70\n",
      " Current loss 0.320349484682\n",
      "\n",
      "Epoch number 70\n",
      " Current loss 0.322486609221\n",
      "\n",
      "Epoch number 70\n",
      " Current loss 0.320533663034\n",
      "\n",
      "Epoch number 70\n",
      " Current loss 0.32149708271\n",
      "\n",
      "Epoch number 71\n",
      " Current loss 0.321001708508\n",
      "\n",
      "Epoch number 71\n",
      " Current loss 0.319899201393\n",
      "\n",
      "Epoch number 71\n",
      " Current loss 0.324984014034\n",
      "\n",
      "Epoch number 71\n",
      " Current loss 0.32372161746\n",
      "\n",
      "Epoch number 72\n",
      " Current loss 0.322424918413\n",
      "\n",
      "Epoch number 72\n",
      " Current loss 0.3200096488\n",
      "\n",
      "Epoch number 72\n",
      " Current loss 0.321835964918\n",
      "\n",
      "Epoch number 72\n",
      " Current loss 0.325873464346\n",
      "\n",
      "Epoch number 73\n",
      " Current loss 0.319678544998\n",
      "\n",
      "Epoch number 73\n",
      " Current loss 0.321913212538\n",
      "\n",
      "Epoch number 73\n",
      " Current loss 0.320949226618\n",
      "\n",
      "Epoch number 73\n",
      " Current loss 0.321586370468\n",
      "\n",
      "Epoch number 74\n",
      " Current loss 0.320769995451\n",
      "\n",
      "Epoch number 74\n",
      " Current loss 0.322825402021\n",
      "\n",
      "Epoch number 74\n",
      " Current loss 0.320248842239\n",
      "\n",
      "Epoch number 74\n",
      " Current loss 0.321199387312\n",
      "\n",
      "Epoch number 75\n",
      " Current loss 0.320652186871\n",
      "\n",
      "Epoch number 75\n",
      " Current loss 0.321019500494\n",
      "\n",
      "Epoch number 75\n",
      " Current loss 0.321073830128\n",
      "\n",
      "Epoch number 75\n",
      " Current loss 0.31973990798\n",
      "\n",
      "Epoch number 76\n",
      " Current loss 0.321156650782\n",
      "\n",
      "Epoch number 76\n",
      " Current loss 0.319129407406\n",
      "\n",
      "Epoch number 76\n",
      " Current loss 0.319161176682\n",
      "\n",
      "Epoch number 76\n",
      " Current loss 0.319691061974\n",
      "\n",
      "Epoch number 77\n",
      " Current loss 0.319977253675\n",
      "\n",
      "Epoch number 77\n",
      " Current loss 0.321470499039\n",
      "\n",
      "Epoch number 77\n",
      " Current loss 0.321611762047\n",
      "\n",
      "Epoch number 77\n",
      " Current loss 0.324423581362\n",
      "\n",
      "Epoch number 78\n",
      " Current loss 0.319413810968\n",
      "\n",
      "Epoch number 78\n",
      " Current loss 0.319474428892\n",
      "\n",
      "Epoch number 78\n",
      " Current loss 0.323217630386\n",
      "\n",
      "Epoch number 78\n",
      " Current loss 0.321040511131\n",
      "\n",
      "Epoch number 79\n",
      " Current loss 0.319437116385\n",
      "\n",
      "Epoch number 79\n",
      " Current loss 0.319634497166\n",
      "\n",
      "Epoch number 79\n",
      " Current loss 0.320564389229\n",
      "\n",
      "Epoch number 79\n",
      " Current loss 0.325326263905\n",
      "\n",
      "Epoch number 80\n",
      " Current loss 0.319762200117\n",
      "\n",
      "Epoch number 80\n",
      " Current loss 0.319649010897\n",
      "\n",
      "Epoch number 80\n",
      " Current loss 0.32019752264\n",
      "\n",
      "Epoch number 80\n",
      " Current loss 0.318798810244\n",
      "\n",
      "Epoch number 81\n",
      " Current loss 0.318416982889\n",
      "\n",
      "Epoch number 81\n",
      " Current loss 0.31988337636\n",
      "\n",
      "Epoch number 81\n",
      " Current loss 0.319066226482\n",
      "\n",
      "Epoch number 81\n",
      " Current loss 0.319265633821\n",
      "\n",
      "Epoch number 82\n",
      " Current loss 0.320955514908\n",
      "\n",
      "Epoch number 82\n",
      " Current loss 0.319623976946\n",
      "\n",
      "Epoch number 82\n",
      " Current loss 0.326657950878\n",
      "\n",
      "Epoch number 82\n",
      " Current loss 0.319281160831\n",
      "\n",
      "Epoch number 83\n",
      " Current loss 0.318916678429\n",
      "\n",
      "Epoch number 83\n",
      " Current loss 0.319222211838\n",
      "\n",
      "Epoch number 83\n",
      " Current loss 0.320439815521\n",
      "\n",
      "Epoch number 83\n",
      " Current loss 0.319497138262\n",
      "\n",
      "Epoch number 84\n",
      " Current loss 0.31950879097\n",
      "\n",
      "Epoch number 84\n",
      " Current loss 0.319029182196\n",
      "\n",
      "Epoch number 84\n",
      " Current loss 0.319442510605\n",
      "\n",
      "Epoch number 84\n",
      " Current loss 0.321321129799\n",
      "\n",
      "Epoch number 85\n",
      " Current loss 0.32041451335\n",
      "\n",
      "Epoch number 85\n",
      " Current loss 0.319365262985\n",
      "\n",
      "Epoch number 85\n",
      " Current loss 0.318726509809\n",
      "\n",
      "Epoch number 85\n",
      " Current loss 0.319305926561\n",
      "\n",
      "Epoch number 86\n",
      " Current loss 0.319702714682\n",
      "\n",
      "Epoch number 86\n",
      " Current loss 0.319085627794\n",
      "\n",
      "Epoch number 86\n",
      " Current loss 0.324151009321\n",
      "\n",
      "Epoch number 86\n",
      " Current loss 0.320461928844\n",
      "\n",
      "Epoch number 87\n",
      " Current loss 0.320482224226\n",
      "\n",
      "Epoch number 87\n",
      " Current loss 0.31861397624\n",
      "\n",
      "Epoch number 87\n",
      " Current loss 0.318468868732\n",
      "\n",
      "Epoch number 87\n",
      " Current loss 0.318715691566\n",
      "\n",
      "Epoch number 88\n",
      " Current loss 0.321546852589\n",
      "\n",
      "Epoch number 88\n",
      " Current loss 0.318113774061\n",
      "\n",
      "Epoch number 88\n",
      " Current loss 0.318921923637\n",
      "\n",
      "Epoch number 88\n",
      " Current loss 0.317367881536\n",
      "\n",
      "Epoch number 89\n",
      " Current loss 0.317922741175\n",
      "\n",
      "Epoch number 89\n",
      " Current loss 0.319361031055\n",
      "\n",
      "Epoch number 89\n",
      " Current loss 0.318814188242\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 89\n",
      " Current loss 0.31816098094\n",
      "\n",
      "Epoch number 90\n",
      " Current loss 0.319735914469\n",
      "\n",
      "Epoch number 90\n",
      " Current loss 0.319433897734\n",
      "\n",
      "Epoch number 90\n",
      " Current loss 0.317344993353\n",
      "\n",
      "Epoch number 90\n",
      " Current loss 0.319764018059\n",
      "\n",
      "Epoch number 91\n",
      " Current loss 0.318590819836\n",
      "\n",
      "Epoch number 91\n",
      " Current loss 0.318406075239\n",
      "\n",
      "Epoch number 91\n",
      " Current loss 0.317612886429\n",
      "\n",
      "Epoch number 91\n",
      " Current loss 0.318080127239\n",
      "\n",
      "Epoch number 92\n",
      " Current loss 0.320365190506\n",
      "\n",
      "Epoch number 92\n",
      " Current loss 0.318578481674\n",
      "\n",
      "Epoch number 92\n",
      " Current loss 0.318559765816\n",
      "\n",
      "Epoch number 92\n",
      " Current loss 0.319631278515\n",
      "\n",
      "Epoch number 93\n",
      " Current loss 0.319637715816\n",
      "\n",
      "Epoch number 93\n",
      " Current loss 0.319186240435\n",
      "\n",
      "Epoch number 93\n",
      " Current loss 0.317769944668\n",
      "\n",
      "Epoch number 93\n",
      " Current loss 0.317944824696\n",
      "\n",
      "Epoch number 94\n",
      " Current loss 0.317716240883\n",
      "\n",
      "Epoch number 94\n",
      " Current loss 0.317931175232\n",
      "\n",
      "Epoch number 94\n",
      " Current loss 0.319959372282\n",
      "\n",
      "Epoch number 94\n",
      " Current loss 0.319174855947\n",
      "\n",
      "Epoch number 95\n",
      " Current loss 0.31737062335\n",
      "\n",
      "Epoch number 95\n",
      " Current loss 0.317386507988\n",
      "\n",
      "Epoch number 95\n",
      " Current loss 0.318286687136\n",
      "\n",
      "Epoch number 95\n",
      " Current loss 0.319800585508\n",
      "\n",
      "Epoch number 96\n",
      " Current loss 0.317727893591\n",
      "\n",
      "Epoch number 96\n",
      " Current loss 0.319914013147\n",
      "\n",
      "Epoch number 96\n",
      " Current loss 0.319146692753\n",
      "\n",
      "Epoch number 96\n",
      " Current loss 0.31898099184\n",
      "\n",
      "Epoch number 97\n",
      " Current loss 0.318206846714\n",
      "\n",
      "Epoch number 97\n",
      " Current loss 0.318798333406\n",
      "\n",
      "Epoch number 97\n",
      " Current loss 0.317365527153\n",
      "\n",
      "Epoch number 97\n",
      " Current loss 0.31830856204\n",
      "\n",
      "Epoch number 98\n",
      " Current loss 0.317319869995\n",
      "\n",
      "Epoch number 98\n",
      " Current loss 0.317350417376\n",
      "\n",
      "Epoch number 98\n",
      " Current loss 0.317621856928\n",
      "\n",
      "Epoch number 98\n",
      " Current loss 0.317731529474\n",
      "\n",
      "Epoch number 99\n",
      " Current loss 0.318765699863\n",
      "\n",
      "Epoch number 99\n",
      " Current loss 0.317451089621\n",
      "\n",
      "Epoch number 99\n",
      " Current loss 0.317544341087\n",
      "\n",
      "Epoch number 99\n",
      " Current loss 0.31801956892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(trainloader,0):\n",
    "        img0, img1, label = data \n",
    "        img0, img1 , label = Variable(img0).cuda(), Variable(img1).cuda() , Variable(label).cuda()\n",
    "        \n",
    "        output = net(img0,img1)\n",
    "        output = torch.cat((output, 1-output), 1)\n",
    "        #output is of the form [similarity, 1-similarity]\n",
    "        #closer to [1, 0] is same face, [0, 1] is different face.\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion.forward(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0 :\n",
    "            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss.data[0]))\n",
    "            iteration_number += 10\n",
    "            counter.append(iteration_number)\n",
    "            loss_history.append(loss.data[0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb29cae0b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_plot(counter,loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = LFWDataset(test=True,\n",
    "                     transform=transforms.Compose([transforms.Scale((128, 128)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ]))\n",
    "testloader = DataLoader(testset, batch_size=8, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "right=wrong=0\n",
    "\n",
    "testnet = SiameseNet().cuda()\n",
    "testnet.load_state_dict(torch.load(\"model\"))\n",
    "\n",
    "\n",
    "for i, data in enumerate(testloader,0):\n",
    "    im1, im2, label = data\n",
    "    label = label[0]\n",
    "#     concatenated = torch.cat((im1,im2),0)\n",
    "    \n",
    "    output = testnet(Variable(im1).cuda(),Variable(im2).cuda())\n",
    "    \n",
    "    for el in output:\n",
    "        if el.data[0] > 0.5:\n",
    "            #predict same face\n",
    "            if label == 1:\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        else:\n",
    "            #predict different\n",
    "            if label == 0:\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(514, 486, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(right, wrong, right+wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
